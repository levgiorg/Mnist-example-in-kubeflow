# PIPELINE DEFINITION
# Name: digits-recognizer-mlflow-pipeline
# Description: MNIST digit recognition pipeline with MLflow integration
# Inputs:
#    mlflow_experiment: str [Default: 'digits-recognizer-kfp']
#    no_epochs: int [Default: 3.0]
#    optimizer: str [Default: 'adam']
# Outputs:
#    model-building-with-mlflow-model_metrics: system.Metrics
components:
  comp-get-data-batch:
    executorLabel: exec-get-data-batch
    outputDefinitions:
      artifacts:
        dataset_summary:
          artifactType:
            schemaTitle: system.Markdown
            schemaVersion: 0.0.1
      parameters:
        dataset_version:
          parameterType: STRING
        test_samples:
          parameterType: NUMBER_DOUBLE
        train_samples:
          parameterType: NUMBER_DOUBLE
  comp-get-latest-data:
    executorLabel: exec-get-latest-data
  comp-model-building-with-mlflow:
    executorLabel: exec-model-building-with-mlflow
    inputDefinitions:
      parameters:
        experiment_name:
          defaultValue: digits-recognizer
          description: MLflow experiment name for tracking
          isOptional: true
          parameterType: STRING
        no_epochs:
          description: Number of training epochs
          parameterType: NUMBER_INTEGER
        optimizer:
          description: Optimizer to use (e.g., 'adam', 'sgd')
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model_view:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        mlflow_run_id:
          parameterType: STRING
        output_model_accuracy:
          parameterType: NUMBER_DOUBLE
        output_model_loss:
          parameterType: NUMBER_DOUBLE
  comp-model-serving:
    executorLabel: exec-model-serving
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-reshape-data:
    executorLabel: exec-reshape-data
    outputDefinitions:
      artifacts:
        reshape_summary:
          artifactType:
            schemaTitle: system.Markdown
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-get-data-batch:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_data_batch
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_data_batch() -> NamedTuple(\n    \"Outputs\",\n    [\n  \
          \      (\"dataset_summary\", Markdown), \n        (\"train_samples\", float),\n\
          \        (\"test_samples\", float),\n        (\"dataset_version\", str),\n\
          \    ]\n):\n    \"\"\"\n    Downloads MNIST dataset using Keras and uploads\
          \ arrays to MinIO storage.\n\n    Returns:\n        NamedTuple containing\
          \ dataset summary markdown, train/test sample counts,\n        and dataset\
          \ version.\n    \"\"\"\n    import json\n    import os\n    import numpy\
          \ as np\n    from tensorflow import keras\n    from minio import Minio\n\
          \n    print(\"Getting data using Keras...\")\n\n    # Download data using\
          \ Keras\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\
          \    print(f\"Data loaded: x_train shape: {x_train.shape}, y_train shape:\
          \ {y_train.shape}\")\n\n    # Connect to MinIO\n    minio_client = Minio(\n\
          \        \"localhost:9000\",  # In production, use env vars\n        access_key=\"\
          minio\",\n        secret_key=\"your-secret-key\",\n        secure=False\n\
          \    )\n    minio_bucket = \"mlpipeline\"\n\n    # Define paths\n    local_tmp_dir\
          \ = \"/tmp\"\n    data_paths = {\n        \"x_train\": \"x_train\",\n  \
          \      \"y_train\": \"y_train\",\n        \"x_test\": \"x_test\",\n    \
          \    \"y_test\": \"y_test\"\n    }\n\n    local_paths = {k: os.path.join(local_tmp_dir,\
          \ v + \".npy\") for k, v in data_paths.items()}\n\n    # Save arrays locally\
          \ and upload to MinIO\n    print(\"Uploading data to MinIO...\")\n\n   \
          \ for data_key, array in [\n        (\"x_train\", x_train),\n        (\"\
          y_train\", y_train),\n        (\"x_test\", x_test),\n        (\"y_test\"\
          , y_test)\n    ]:\n        local_path = local_paths[data_key]\n        remote_path\
          \ = data_paths[data_key]\n\n        # Save locally\n        np.save(local_path,\
          \ array)\n\n        # Upload to MinIO\n        minio_client.fput_object(minio_bucket,\
          \ remote_path, local_path)\n        print(f\"Uploaded {data_key} to {minio_bucket}/{remote_path}\"\
          )\n\n        # Clean up\n        os.remove(local_path)\n\n    # Prepare\
          \ output metadata\n    train_count = float(x_train.shape[0])\n    test_count\
          \ = float(x_test.shape[0])\n    dataset_version = \"1.0\"\n\n    # Create\
          \ markdown content\n    markdown_summary = f\"\"\"\n# MNIST Dataset Overview\n\
          \n* **Source:** Keras Datasets (`keras.datasets.mnist.load_data()`)\n* **Version:**\
          \ {dataset_version}\n* **Training Samples:** {int(train_count)}\n* **Test\
          \ Samples:** {int(test_count)}\n* **Data Shapes:**\n  * Training images:\
          \ `{x_train.shape}`\n  * Training labels: `{y_train.shape}`\n  * Test images:\
          \ `{x_test.shape}`\n  * Test labels: `{y_test.shape}`\n* **Storage:** MinIO\
          \ bucket `{minio_bucket}`\n\"\"\"\n\n    # Format for KFP markdown artifact\n\
          \    markdown_artifact_json = {\n        'outputs': [{\n            'type':\
          \ 'markdown',\n            'storage': 'inline',\n            'source': markdown_summary\n\
          \        }]\n    }\n\n    return (\n        json.dumps(markdown_artifact_json),\n\
          \        train_count,\n        test_count,\n        dataset_version\n  \
          \  )\n\n"
        image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0
    exec-get-latest-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_latest_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_latest_data():\n    \"\"\"\n    Placeholder component for\
          \ adding latest data to the dataset.\n\n    In a real-world scenario, this\
          \ could fetch new data from a database,\n    API, or other data source and\
          \ merge with existing data.\n    \"\"\"\n    print(\"Component for adding\
          \ latest data (placeholder)\")\n    # Actual implementation would fetch\
          \ and integrate new data\n\n"
        image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0
    exec-model-building-with-mlflow:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_building_with_mlflow
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'mlflow>=2.0.0'\
          \ 'boto3' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_building_with_mlflow(\n    no_epochs: int,\n    optimizer:\
          \ str,\n    experiment_name: str = \"digits-recognizer\",\n) -> NamedTuple(\n\
          \    \"Outputs\",\n    [\n        (\"model_view\", Artifact),\n        (\"\
          model_metrics\", Metrics),\n        (\"output_model_accuracy\", float),\n\
          \        (\"output_model_loss\", float),\n        (\"mlflow_run_id\", str)\n\
          \    ]\n):\n    \"\"\"\n    Build, train, and evaluate a CNN model for digit\
          \ recognition with MLflow tracking.\n\n    This component:\n    - Creates\
          \ a CNN model using TensorFlow/Keras\n    - Logs model architecture, parameters,\
          \ and training config to MLflow\n    - Trains the model on the preprocessed\
          \ MNIST data\n    - Evaluates model performance on test data\n    - Generates\
          \ confusion matrix and performance visualizations\n    - Saves model artifacts\
          \ to MinIO for deployment\n    - Registers the model in MLflow Model Registry\n\
          \n    Args:\n        no_epochs: Number of training epochs\n        optimizer:\
          \ Optimizer to use (e.g., 'adam', 'sgd')\n        experiment_name: MLflow\
          \ experiment name for tracking\n\n    Returns:\n        NamedTuple containing\
          \ model visualization, metrics, accuracy, loss, \n        and MLflow run\
          \ ID.\n    \"\"\"\n    import json\n    import os\n    import shutil\n \
          \   import numpy as np\n    import pandas as pd\n    import tensorflow as\
          \ tf\n    from tensorflow import keras\n    from minio import Minio\n  \
          \  import mlflow\n    import mlflow.tensorflow\n    import boto3\n\n   \
          \ print(f\"Building model with: epochs={no_epochs}, optimizer={optimizer}\"\
          )\n\n    # ===== MLflow Setup =====\n    # Configure environment variables\n\
          \    os.environ[\"MLFLOW_TRACKING_URI\"] = os.environ.get(\n        \"MLFLOW_TRACKING_URI\"\
          , \"http://mlflow-server.kubeflow.svc.cluster.local:5000\"\n    )\n    os.environ[\"\
          MLFLOW_S3_ENDPOINT_URL\"] = os.environ.get(\n        \"MLFLOW_S3_ENDPOINT_URL\"\
          , \"http://localhost:9000\"\n    )\n    os.environ[\"AWS_ACCESS_KEY_ID\"\
          ] = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"minio\")\n    os.environ[\"\
          AWS_SECRET_ACCESS_KEY\"] = os.environ.get(\n        \"AWS_SECRET_ACCESS_KEY\"\
          , \"your-secret-key\"\n    )\n\n    # Ensure MLflow bucket exists\n    default_bucket_name\
          \ = \"mlflow\"\n    s3_client = boto3.client(\n        \"s3\",\n       \
          \ endpoint_url=os.environ[\"MLFLOW_S3_ENDPOINT_URL\"],\n        aws_access_key_id=os.environ[\"\
          AWS_ACCESS_KEY_ID\"],\n        aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"\
          ],\n        config=boto3.session.Config(signature_version=\"s3v4\"),\n \
          \   )\n\n    try:\n        buckets_response = s3_client.list_buckets()\n\
          \        existing_buckets = [bucket[\"Name\"] for bucket in buckets_response[\"\
          Buckets\"]]\n\n        if default_bucket_name not in existing_buckets:\n\
          \            print(f\"Creating MLflow bucket: {default_bucket_name}\")\n\
          \            s3_client.create_bucket(Bucket=default_bucket_name)\n    except\
          \ Exception as e:\n        print(f\"Warning: MLflow bucket check failed:\
          \ {e}\")\n\n    # Create or get MLflow experiment\n    try:\n        experiment\
          \ = mlflow.get_experiment_by_name(experiment_name)\n        if experiment\
          \ is None:\n            experiment_id = mlflow.create_experiment(experiment_name)\n\
          \            print(f\"Created MLflow experiment: {experiment_name} (ID:\
          \ {experiment_id})\")\n        else:\n            experiment_id = experiment.experiment_id\n\
          \            print(f\"Using existing MLflow experiment: {experiment_name}\
          \ (ID: {experiment_id})\")\n    except Exception as e:\n        print(f\"\
          MLflow experiment setup issue: {e}\")\n        experiment_id = None\n\n\
          \    # ===== Data Loading Setup =====\n    minio_client = Minio(\n     \
          \   \"localhost:9000\",\n        access_key=\"minio\",\n        secret_key=\"\
          your-secret-key\",\n        secure=False\n    )\n    minio_bucket = \"mlpipeline\"\
          \n    local_tmp_dir = \"/tmp\"\n\n    # Define storage paths\n    storage_paths\
          \ = {\n        \"x_train\": \"x_train\",\n        \"y_train\": \"y_train\"\
          , \n        \"x_test\": \"x_test\",\n        \"y_test\": \"y_test\"\n  \
          \  }\n\n    local_paths = {k: os.path.join(local_tmp_dir, v + \".npy\")\
          \ for k, v in storage_paths.items()}\n    model_save_path = os.path.join(local_tmp_dir,\
          \ \"detect-digits\")\n    model_upload_prefix = \"models/detect-digits/1\"\
          \n\n    # ===== Model Definition =====\n    print(\"Creating CNN model for\
          \ digit recognition...\")\n    model = keras.models.Sequential([\n     \
          \   # Convolutional layer\n        keras.layers.Conv2D(\n            filters=64,\
          \ \n            kernel_size=(3, 3),\n            activation='relu',\n  \
          \          input_shape=(28, 28, 1)\n        ),\n        keras.layers.MaxPool2D(pool_size=(2,\
          \ 2)),\n\n        # Flatten for dense layers\n        keras.layers.Flatten(),\n\
          \n        # Dense layers\n        keras.layers.Dense(64, activation='relu'),\n\
          \        keras.layers.Dense(32, activation='relu'),\n\n        # Output\
          \ layer (10 classes for digits 0-9)\n        keras.layers.Dense(10, activation='softmax')\n\
          \    ])\n\n    # Capture model summary\n    model_summary_lines = []\n \
          \   model.summary(print_fn=lambda x: model_summary_lines.append(x))\n  \
          \  model_summary_text = \"\\n\".join(model_summary_lines)\n    print(model_summary_text)\n\
          \n    # Compile model\n    model.compile(\n        optimizer=optimizer,\n\
          \        loss=\"sparse_categorical_crossentropy\",\n        metrics=['accuracy']\n\
          \    )\n\n    # ===== Load Training Data =====\n    print(\"Loading training\
          \ data from MinIO...\")\n    try:\n        # Load training data\n      \
          \  minio_client.fget_object(minio_bucket, storage_paths[\"x_train\"], local_paths[\"\
          x_train\"])\n        x_train = np.load(local_paths[\"x_train\"])\n\n   \
          \     minio_client.fget_object(minio_bucket, storage_paths[\"y_train\"],\
          \ local_paths[\"y_train\"])\n        y_train = np.load(local_paths[\"y_train\"\
          ])\n\n        print(f\"Loaded x_train shape: {x_train.shape}, y_train shape:\
          \ {y_train.shape}\")\n    except Exception as e:\n        print(f\"Error\
          \ loading training data: {e}\")\n        raise\n\n    # ===== Start MLflow\
          \ Run =====\n    with mlflow.start_run(experiment_id=experiment_id) as run:\n\
          \        mlflow_run_id = run.info.run_id\n        print(f\"Started MLflow\
          \ run: {mlflow_run_id}\")\n\n        # Log model parameters\n        mlflow.log_params({\n\
          \            \"epochs\": no_epochs,\n            \"optimizer\": optimizer,\n\
          \            \"model_type\": \"CNN\",\n            \"input_shape\": str((28,\
          \ 28, 1)),\n            \"output_classes\": 10,\n            \"conv_filters\"\
          : 64,\n            \"dense_units\": \"64,32,10\",\n            \"batch_size\"\
          : 20,\n            \"activation\": \"relu,relu,softmax\"\n        })\n\n\
          \        # Log model architecture\n        mlflow.log_text(model_summary_text,\
          \ \"model_architecture.txt\")\n\n        # ===== Train Model =====\n   \
          \     print(f\"Training model for {no_epochs} epochs...\")\n        try:\n\
          \            history = model.fit(\n                x=x_train,\n        \
          \        y=y_train,\n                epochs=no_epochs,\n               \
          \ batch_size=20,\n                verbose=2  # Show progress for each epoch\n\
          \            )\n            print(\"Training completed successfully\")\n\
          \n            # Log training history\n            for epoch, (loss, acc)\
          \ in enumerate(zip(\n                history.history['loss'], \n       \
          \         history.history['accuracy']\n            )):\n               \
          \ mlflow.log_metrics({\n                    \"train_loss\": loss,\n    \
          \                \"train_accuracy\": acc\n                }, step=epoch)\n\
          \n        except Exception as e:\n            print(f\"Error during model\
          \ training: {e}\")\n            raise\n\n        # ===== Load Test Data\
          \ for Evaluation =====\n        print(\"Loading test data for evaluation...\"\
          )\n        try:\n            # Load test data\n            minio_client.fget_object(minio_bucket,\
          \ storage_paths[\"x_test\"], local_paths[\"x_test\"])\n            x_test\
          \ = np.load(local_paths[\"x_test\"])\n\n            minio_client.fget_object(minio_bucket,\
          \ storage_paths[\"y_test\"], local_paths[\"y_test\"])\n            y_test\
          \ = np.load(local_paths[\"y_test\"])\n\n            print(f\"Loaded x_test\
          \ shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n        except\
          \ Exception as e:\n            print(f\"Error loading test data: {e}\")\n\
          \            raise\n\n        # ===== Evaluate Model =====\n        print(\"\
          Evaluating model on test data...\")\n        test_loss, test_accuracy =\
          \ model.evaluate(x=x_test, y=y_test, verbose=0)\n        print(f\"Test accuracy:\
          \ {test_accuracy:.4f}, Test loss: {test_loss:.4f}\")\n\n        # Log test\
          \ metrics to MLflow\n        mlflow.log_metrics({\n            \"test_accuracy\"\
          : test_accuracy,\n            \"test_loss\": test_loss\n        })\n\n \
          \       # ===== Generate Confusion Matrix =====\n        print(\"Generating\
          \ confusion matrix...\")\n        test_predictions = model.predict(x=x_test)\n\
          \        predicted_classes = np.argmax(test_predictions, axis=1)\n\n   \
          \     confusion_matrix = tf.math.confusion_matrix(\n            labels=y_test,\
          \ \n            predictions=predicted_classes\n        ).numpy()\n\n   \
          \     # Format confusion matrix for visualization\n        labels = list(map(str,\
          \ range(10)))  # 0-9 digits\n        confusion_data = []\n\n        for\
          \ true_idx, row in enumerate(confusion_matrix):\n            for pred_idx,\
          \ count in enumerate(row):\n                confusion_data.append((labels[true_idx],\
          \ labels[pred_idx], int(count)))\n\n        df_confusion = pd.DataFrame(\n\
          \            confusion_data, \n            columns=['target', 'predicted',\
          \ 'count']\n        )\n\n        # Save confusion matrix for MLflow\n  \
          \      cm_file_path = os.path.join(local_tmp_dir, \"confusion_matrix.csv\"\
          )\n        df_confusion.to_csv(cm_file_path, index=False)\n        mlflow.log_artifact(cm_file_path,\
          \ \"confusion_matrix\")\n\n        # Format confusion matrix for KFP UI\n\
          \        cm_csv = df_confusion.to_csv(header=False, index=False)\n\n   \
          \     # ===== Save Model =====\n        print(f\"Saving model to {model_save_path}...\"\
          )\n        if os.path.exists(model_save_path):\n            shutil.rmtree(model_save_path)\n\
          \n        os.makedirs(model_save_path)\n        keras.models.save_model(model,\
          \ model_save_path)\n\n        # Log model to MLflow\n        mlflow.tensorflow.log_model(\n\
          \            model, \n            artifact_path=\"model\",\n           \
          \ registered_model_name=\"digits-recognizer-model\"\n        )\n       \
          \ print(\"Model saved and logged to MLflow\")\n\n        # ===== Upload\
          \ Model to MinIO =====\n        print(f\"Uploading model to MinIO: {minio_bucket}/{model_upload_prefix}\"\
          )\n\n        # Helper function to upload directory recursively\n       \
          \ def upload_directory_to_minio(local_dir, bucket, prefix):\n          \
          \  \"\"\"Upload directory contents recursively to MinIO\"\"\"\n        \
          \    for root, _, files in os.walk(local_dir):\n                for file\
          \ in files:\n                    # Get full path\n                    local_file_path\
          \ = os.path.join(root, file)\n\n                    # Get relative path\
          \ from the source directory\n                    rel_path = os.path.relpath(local_file_path,\
          \ local_dir)\n\n                    # Construct MinIO object path with normalized\
          \ separators\n                    minio_key = f\"{prefix}/{rel_path}\".replace(\"\
          \\\\\", \"/\")\n\n                    print(f\"  Uploading: {local_file_path}\
          \ \u2192 {bucket}/{minio_key}\")\n                    try:\n           \
          \             minio_client.fput_object(bucket, minio_key, local_file_path)\n\
          \                    except Exception as e:\n                        print(f\"\
          \  Error uploading {local_file_path}: {e}\")\n                        raise\n\
          \n        try:\n            upload_directory_to_minio(model_save_path, minio_bucket,\
          \ model_upload_prefix)\n            print(\"Model successfully uploaded\
          \ to MinIO\")\n        except Exception as e:\n            print(f\"Error\
          \ uploading model to MinIO: {e}\")\n            raise\n\n        # =====\
          \ Prepare KFP UI Outputs =====\n\n        # Combined view: confusion matrix\
          \ + markdown report\n        model_view_json = {\n            \"outputs\"\
          : [\n                # Confusion matrix visualization\n                {\n\
          \                    \"type\": \"confusion_matrix\",\n                 \
          \   \"format\": \"csv\",\n                    \"schema\": [\n          \
          \              {'name': 'target', 'type': 'CATEGORY'},\n               \
          \         {'name': 'predicted', 'type': 'CATEGORY'},\n                 \
          \       {'name': 'count', 'type': 'NUMBER'},\n                    ],\n \
          \                   \"source\": cm_csv,\n                    \"storage\"\
          : \"inline\",\n                    \"labels\": labels\n                },\n\
          \                # Markdown report\n                {\n                \
          \    'storage': 'inline',\n                    'source': f'''# Model Training\
          \ & Evaluation Report\n\n## Training Configuration\n* **Epochs:** {no_epochs}\n\
          * **Optimizer:** {optimizer}\n* **Batch Size:** 20\n* **Loss Function:**\
          \ sparse_categorical_crossentropy\n\n## MLflow Tracking\n* **Experiment:**\
          \ {experiment_name}\n* **Run ID:** {mlflow_run_id}\n* **Tracking Server:**\
          \ {os.environ[\"MLFLOW_TRACKING_URI\"]}\n\n## Model Architecture\n```\n\
          {model_summary_text}\n```\n\n## Evaluation on Test Set\n* **Accuracy:**\
          \ {test_accuracy:.4f}\n* **Loss:** {test_loss:.4f}\n\n## Model Storage\n\
          * **MinIO Path:** {minio_bucket}/{model_upload_prefix}\n* **Registered in\
          \ MLflow:** Yes (as \"digits-recognizer-model\")\n''',\n               \
          \     'type': 'markdown',\n                }\n            ]\n        }\n\
          \n        # Metrics for KFP UI\n        metrics_json = {\n            'metrics':\
          \ [\n                {\n                    'name': 'model-accuracy',\n\
          \                    'numberValue': float(test_accuracy),\n            \
          \        'format': \"PERCENTAGE\"\n                },\n                {\n\
          \                    'name': 'model-loss',\n                    'numberValue':\
          \ float(test_loss),\n                    'format': \"RAW\"\n           \
          \     }\n            ]\n        }\n\n    # ===== Clean Up =====\n    print(\"\
          Cleaning up temporary files...\")\n    for path in local_paths.values():\n\
          \        if os.path.exists(path):\n            os.remove(path)\n\n    if\
          \ os.path.exists(model_save_path):\n        shutil.rmtree(model_save_path)\n\
          \n    if os.path.exists(cm_file_path):\n        os.remove(cm_file_path)\n\
          \n    return (\n        json.dumps(model_view_json),\n        json.dumps(metrics_json),\n\
          \        float(test_accuracy),\n        float(test_loss),\n        mlflow_run_id\n\
          \    )\n\n"
        image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0
    exec-model-serving:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_serving
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kserve==0.8.0.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_serving() -> str:\n    \"\"\"\n    Create a KServe InferenceService\
          \ for the trained model.\n\n    This component:\n    - Configures Kubernetes\
          \ client for the current environment\n    - Creates a KServe InferenceService\
          \ definition with appropriate settings\n    - Deploys the service to make\
          \ the model available for inference\n\n    Returns:\n        The name of\
          \ the created KServe InferenceService\n    \"\"\"\n    import os\n    from\
          \ datetime import datetime\n    from kubernetes import client, config\n\
          \    from kserve import KServeClient\n    from kserve import constants\n\
          \    from kserve import utils\n    from kserve import V1beta1InferenceService\n\
          \    from kserve import V1beta1InferenceServiceSpec\n    from kserve import\
          \ V1beta1PredictorSpec\n    from kserve import V1beta1TFServingSpec\n\n\
          \    print(\"Setting up model serving with KServe...\")\n\n    # ===== Kubernetes\
          \ Configuration =====\n    try:\n        # Try in-cluster config first (when\
          \ running in Kubeflow)\n        config.load_incluster_config()\n       \
          \ print(\"Using in-cluster Kubernetes configuration\")\n    except config.ConfigException:\n\
          \        try:\n            # Fall back to local kubeconfig\n           \
          \ config.load_kube_config()\n            print(\"Using local Kubernetes\
          \ configuration (kubeconfig)\")\n        except config.ConfigException as\
          \ e:\n            print(f\"ERROR: Could not configure Kubernetes client:\
          \ {e}\")\n            raise\n\n    # ===== Determine Namespace =====\n \
          \   namespace = utils.get_default_target_namespace()\n    if not namespace:\n\
          \        # Try to get namespace from service account\n        sa_namespace_path\
          \ = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'\n        try:\n\
          \            with open(sa_namespace_path, 'r') as f:\n                namespace\
          \ = f.read().strip()\n            print(f\"Using namespace from service\
          \ account: {namespace}\")\n        except FileNotFoundError:\n         \
          \   namespace = 'kubeflow'  # Default namespace\n            print(f\"Using\
          \ default namespace: {namespace}\")\n\n    # ===== Service Configuration\
          \ =====\n    minio_bucket = os.environ.get('MINIO_BUCKET', 'mlpipeline')\n\
          \    storage_uri = f\"s3://{minio_bucket}/models/detect-digits\"\n    service_account\
          \ = os.environ.get('KSERVE_SA_NAME', \"sa-minio-kserve\")\n\n    # Generate\
          \ unique service name with timestamp\n    timestamp = datetime.now().strftime(\"\
          %Y%m%d-%H%M%S\")\n    service_name = f'digits-recognizer-{timestamp}'\n\n\
          \    kserve_version = 'v1beta1'\n    api_version = f\"{constants.KSERVE_GROUP}/{kserve_version}\"\
          \n\n    print(f\"Creating KServe InferenceService '{service_name}'\")\n\
          \    print(f\"Model storage URI: {storage_uri}\")\n    print(f\"Service\
          \ account: {service_account}\")\n    print(f\"Namespace: {namespace}\")\n\
          \n    # ===== Create InferenceService Definition =====\n    isvc = V1beta1InferenceService(\n\
          \        api_version=api_version,\n        kind=constants.KSERVE_KIND,\n\
          \        metadata=client.V1ObjectMeta(\n            name=service_name,\n\
          \            namespace=namespace,\n            annotations={\"sidecar.istio.io/inject\"\
          : \"false\"}\n        ),\n        spec=V1beta1InferenceServiceSpec(\n  \
          \          predictor=V1beta1PredictorSpec(\n                service_account_name=service_account,\n\
          \                tensorflow=V1beta1TFServingSpec(\n                    storage_uri=storage_uri,\n\
          \                )\n            )\n        )\n    )\n\n    # ===== Deploy\
          \ Service =====\n    kserve_client = KServeClient()\n    try:\n        print(\"\
          Creating KServe InferenceService...\")\n        kserve_client.create(isvc,\
          \ namespace=namespace)\n        print(f\"Successfully created InferenceService:\
          \ {service_name}\")\n    except Exception as e:\n        print(f\"ERROR\
          \ creating InferenceService: {e}\")\n        try:\n            # Try to\
          \ get status of potentially partially created service\n            status\
          \ = kserve_client.get(service_name, namespace=namespace)\n            print(f\"\
          Current status: {status}\")\n        except Exception as get_e:\n      \
          \      print(f\"Could not get status: {get_e}\")\n        raise\n\n    return\
          \ service_name\n\n"
        image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0
    exec-reshape-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - reshape_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef reshape_data() -> NamedTuple(\"Outputs\", [(\"reshape_summary\"\
          , Markdown)]): \n    \"\"\"\n    Reshape and normalize MNIST data for model\
          \ training.\n\n    - Reshapes data to add channel dimension (28x28x1)\n\
          \    - Normalizes pixel values to [0,1] range\n    - Saves processed data\
          \ back to MinIO\n\n    Returns:\n        NamedTuple containing summary of\
          \ the preprocessing steps.\n    \"\"\"\n    import json\n    import os\n\
          \    import numpy as np\n    from minio import Minio\n\n    print(\"Reshaping\
          \ and normalizing MNIST data...\")\n\n    # Connect to MinIO\n    minio_client\
          \ = Minio(\n        \"localhost:9000\",\n        access_key=\"minio\"\
          ,\n        secret_key=\"your-secret-key\",\n        secure=False\n    )\n\
          \    minio_bucket = \"mlpipeline\"\n    local_tmp_dir = \"/tmp\"\n\n   \
          \ # Define paths\n    minio_paths = {\n        \"x_train\": \"x_train\"\
          ,\n        \"x_test\": \"x_test\"\n    }\n\n    local_paths = {k: os.path.join(local_tmp_dir,\
          \ v + \".npy\") for k, v in minio_paths.items()}\n\n    # Load data from\
          \ MinIO\n    x_train_shape, x_test_shape = None, None\n\n    try:\n    \
          \    # Process training data\n        minio_client.fget_object(minio_bucket,\
          \ minio_paths[\"x_train\"], local_paths[\"x_train\"])\n        x_train =\
          \ np.load(local_paths[\"x_train\"])\n        print(f\"Original x_train shape:\
          \ {x_train.shape}\")\n        x_train_shape = x_train.shape\n\n        #\
          \ Reshape and normalize\n        x_train = x_train.reshape(-1, 28, 28, 1)\n\
          \        x_train = x_train / 255.0\n        print(f\"Processed x_train shape:\
          \ {x_train.shape}\")\n\n        # Save back to MinIO\n        np.save(local_paths[\"\
          x_train\"], x_train)\n        minio_client.fput_object(minio_bucket, minio_paths[\"\
          x_train\"], local_paths[\"x_train\"])\n\n        # Process test data\n \
          \       minio_client.fget_object(minio_bucket, minio_paths[\"x_test\"],\
          \ local_paths[\"x_test\"])\n        x_test = np.load(local_paths[\"x_test\"\
          ])\n        print(f\"Original x_test shape: {x_test.shape}\")\n        x_test_shape\
          \ = x_test.shape\n\n        # Reshape and normalize\n        x_test = x_test.reshape(-1,\
          \ 28, 28, 1)\n        x_test = x_test / 255.0\n        print(f\"Processed\
          \ x_test shape: {x_test.shape}\")\n\n        # Save back to MinIO\n    \
          \    np.save(local_paths[\"x_test\"], x_test)\n        minio_client.fput_object(minio_bucket,\
          \ minio_paths[\"x_test\"], local_paths[\"x_test\"])\n\n    except Exception\
          \ as e:\n        print(f\"Error processing data: {e}\")\n        raise\n\
          \    finally:\n        # Clean up local files\n        for path in local_paths.values():\n\
          \            if os.path.exists(path):\n                os.remove(path)\n\
          \n    # Create markdown summary\n    markdown_summary = f\"\"\"\n# Data\
          \ Preprocessing Summary\n\n## Preprocessing Steps:\n1. **Reshaping:** Added\
          \ channel dimension for CNN input\n   * Training data: {x_train_shape} \u2192\
          \ {(-1, 28, 28, 1)}\n   * Test data: {x_test_shape} \u2192 {(-1, 28, 28,\
          \ 1)}\n2. **Normalization:** Scaled pixel values from [0,255] to [0,1]\n\
          \   * Divided all pixel values by 255.0\n\n## Storage:\n* Processed data\
          \ saved back to MinIO bucket: `{minio_bucket}`\n* Original paths were preserved:\
          \ `{\", \".join(minio_paths.values())}`\n\"\"\"\n\n    markdown_artifact_json\
          \ = {\n        'outputs': [{\n            'type': 'markdown',\n        \
          \    'storage': 'inline',\n            'source': markdown_summary\n    \
          \    }]\n    }\n\n    return (json.dumps(markdown_artifact_json),)\n\n"
        image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0
pipelineInfo:
  description: MNIST digit recognition pipeline with MLflow integration
  name: digits-recognizer-mlflow-pipeline
root:
  dag:
    outputs:
      artifacts:
        model-building-with-mlflow-model_metrics:
          artifactSelectors:
          - outputArtifactKey: model_metrics
            producerSubtask: model-building-with-mlflow
    tasks:
      get-data-batch:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-get-data-batch
        taskInfo:
          name: get-data-batch
      get-latest-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-get-latest-data
        dependentTasks:
        - get-data-batch
        taskInfo:
          name: get-latest-data
      model-building-with-mlflow:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-building-with-mlflow
        dependentTasks:
        - reshape-data
        inputs:
          parameters:
            experiment_name:
              componentInputParameter: mlflow_experiment
            no_epochs:
              componentInputParameter: no_epochs
            optimizer:
              componentInputParameter: optimizer
        taskInfo:
          name: model-building-with-mlflow
      model-serving:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-serving
        dependentTasks:
        - model-building-with-mlflow
        taskInfo:
          name: model-serving
      reshape-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-reshape-data
        dependentTasks:
        - get-data-batch
        taskInfo:
          name: reshape-data
  inputDefinitions:
    parameters:
      mlflow_experiment:
        defaultValue: digits-recognizer-kfp
        description: MLflow experiment name for tracking
        isOptional: true
        parameterType: STRING
      no_epochs:
        defaultValue: 3.0
        description: Number of training epochs
        isOptional: true
        parameterType: NUMBER_INTEGER
      optimizer:
        defaultValue: adam
        description: Optimizer to use for training
        isOptional: true
        parameterType: STRING
  outputDefinitions:
    artifacts:
      model-building-with-mlflow-model_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
